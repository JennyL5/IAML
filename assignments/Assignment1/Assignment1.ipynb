{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning\n",
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "#### <span style=\"color:blue\">SUBMISSION INSTRUCTIONS WILL BE REALEASED SHORTLY</span>\n",
    "\n",
    "**It is important that you carefully follow the instructions below for things to work properly.**\n",
    "\n",
    "1. You need to have your environment set up as in the [README](https://github.com/amosstorkey/iaml-labs) and you need to activate this environment before running this notebook:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation.\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (which typically refers only to the independent variables, i.e. excluding the target variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "This assignment is formative and such will not count towards your final grade. Nonetheless, we ask you to submit answers to certain questions so that you can become familiar with the Gradescope system and so that we can summarize common mistakes people might make.\n",
    "\n",
    "We will be using [Gradescope](https://www.gradescope.com/) for submissions. Submission instructions will be released separately shortly. You will be using a separate Latex-based file where you would copy your answers and/or code.\n",
    "\n",
    "**IMPORTANT: Only specific questions need to be submitted. These are Question 2.2, Question 2.6, Question 4.3 and Question 4.4,**\n",
    "\n",
    "The submission deadline for this assignment by **Monday 14/10/2019 at 16:00**. \n",
    "\n",
    "Since this assignment is formative, there will be no marking assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Execute the cell below to import all packages you will be using in the rest of the assignemnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.plotter import scatter_jitter, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## 20 Newsgroup Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset\n",
    "\n",
    "This question is based on the 20 Newsgroups Dataset. This dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "There are three versions of the 20 Newsgroups Dataset. In this assignment we will use the `bydate` matlab version in which documents are sorted by date into training (60%) and test (40%) sets, newsgroup-identifying headers are dropped and duplicates are removed. This collection comprises roughly 61,000 different words, which results in a bag-of-words representation with frequency counts. More specifically, each document is represented by a 61,000 dimensional vector that contains the counts for each of the 61,000 different words present in the respective document. \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only 5 out of the 20 newsgroups, which results in a 5-class problem. The class is conveniently stored in the `class` column. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes 2-3 and 4-5 are rather closely related. Additionally, we computed the [mutual information](https://en.wikipedia.org/wiki/Mutual_information) of each word with the class attribute and selected the some words out of 61,000 that had highest mutual information. For very sophisticated technical reasons (which you should know!) 1 was added to all the word counts in part 1. The resulting representation is much more compact and can be used directly to perform our experiments in Python.\n",
    "\n",
    "**Hint**: The data was preprocessed by a very busy PhD student... and hence should never be taken to be perfect at face value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to get a feel for the data that you will be dealing with in the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 ==========\n",
    "\n",
    "1. [Code] Load the dataset `raw_20news.csv` into a data-frame called `news_raw`. Using pandas methods we learnt in class, extract some basic information about the data. \n",
    "\n",
    "1. [Text] In a short paragraph, summarise the key features of the dataset. *Hint: Look at what we did in the labs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'raw_20news.csv')\n",
    "nesw_raw = pd.read_csv(data_path, delimiter = ',')\n",
    "#column_names = nesw_raw.columns \n",
    "#indexes = nesw_raw.index\n",
    "#print(column_names)\n",
    "#print(indexes)\n",
    "#nesw_raw\n",
    "#nesw_raw.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .loc indexer selects data in a different way than just the indexing operator. It can select subsets of rows or columns. It can also simultaneously select subsets of rows and columns. Most importantly, it only selects data by the LABEL of the rows and columns.\n",
    "The .iloc indexer is very similar to .loc but only uses integer locations to make its selections. The word .iloc itself stands for integer location so that should help with remember what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 ==========\n",
    "1. [Code] Display the names of some of the attributes in the training datset. \n",
    "1. [Text] Describe the output and comment (1 or 2 sentences) keeping in mind the selection procedure for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1_aaa</th>\n",
       "      <th>w2_pins</th>\n",
       "      <th>w3_kmr</th>\n",
       "      <th>w4_notion</th>\n",
       "      <th>w5_queens</th>\n",
       "      <th>w6_dwyer</th>\n",
       "      <th>w7_defenseman</th>\n",
       "      <th>w8_gld</th>\n",
       "      <th>w9_tocchet</th>\n",
       "      <th>w10_home</th>\n",
       "      <th>...</th>\n",
       "      <th>w512_constantly</th>\n",
       "      <th>w513_generate</th>\n",
       "      <th>w514_definite</th>\n",
       "      <th>w515_lacks</th>\n",
       "      <th>w516_combination</th>\n",
       "      <th>w517_sitting</th>\n",
       "      <th>w518_surface</th>\n",
       "      <th>w519_fashion</th>\n",
       "      <th>w520_sit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.00000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "      <td>2129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.128229</td>\n",
       "      <td>6.097698</td>\n",
       "      <td>6.120244</td>\n",
       "      <td>5.551902</td>\n",
       "      <td>5.521841</td>\n",
       "      <td>6.12776</td>\n",
       "      <td>5.633161</td>\n",
       "      <td>6.090653</td>\n",
       "      <td>5.970409</td>\n",
       "      <td>5.624706</td>\n",
       "      <td>...</td>\n",
       "      <td>9.666510</td>\n",
       "      <td>9.217473</td>\n",
       "      <td>9.061531</td>\n",
       "      <td>9.398309</td>\n",
       "      <td>9.175200</td>\n",
       "      <td>9.708783</td>\n",
       "      <td>8.807891</td>\n",
       "      <td>9.719587</td>\n",
       "      <td>9.307656</td>\n",
       "      <td>3.092532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.292577</td>\n",
       "      <td>46.190953</td>\n",
       "      <td>46.656022</td>\n",
       "      <td>40.953913</td>\n",
       "      <td>40.978098</td>\n",
       "      <td>45.96434</td>\n",
       "      <td>41.146918</td>\n",
       "      <td>45.762060</td>\n",
       "      <td>44.266628</td>\n",
       "      <td>40.769105</td>\n",
       "      <td>...</td>\n",
       "      <td>45.844064</td>\n",
       "      <td>43.948910</td>\n",
       "      <td>40.969185</td>\n",
       "      <td>43.833064</td>\n",
       "      <td>42.403283</td>\n",
       "      <td>47.294120</td>\n",
       "      <td>39.341038</td>\n",
       "      <td>46.185082</td>\n",
       "      <td>45.059367</td>\n",
       "      <td>1.395948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>572.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>600.00000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>587.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>599.000000</td>\n",
       "      <td>585.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>597.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            w1_aaa      w2_pins       w3_kmr    w4_notion    w5_queens  \\\n",
       "count  2129.000000  2129.000000  2129.000000  2129.000000  2129.000000   \n",
       "mean      5.128229     6.097698     6.120244     5.551902     5.521841   \n",
       "std      38.292577    46.190953    46.656022    40.953913    40.978098   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max     572.000000   583.000000   579.000000   580.000000   591.000000   \n",
       "\n",
       "         w6_dwyer  w7_defenseman       w8_gld   w9_tocchet     w10_home  \\\n",
       "count  2129.00000    2129.000000  2129.000000  2129.000000  2129.000000   \n",
       "mean      6.12776       5.633161     6.090653     5.970409     5.624706   \n",
       "std      45.96434      41.146918    45.762060    44.266628    40.769105   \n",
       "min       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.00000       1.000000     1.000000     1.000000     1.000000   \n",
       "max     600.00000     546.000000   591.000000   578.000000   578.000000   \n",
       "\n",
       "          ...       w512_constantly  w513_generate  w514_definite  \\\n",
       "count     ...           2129.000000    2129.000000    2129.000000   \n",
       "mean      ...              9.666510       9.217473       9.061531   \n",
       "std       ...             45.844064      43.948910      40.969185   \n",
       "min       ...              1.000000       1.000000       1.000000   \n",
       "25%       ...              3.000000       2.000000       3.000000   \n",
       "50%       ...              5.000000       5.000000       5.000000   \n",
       "75%       ...              7.000000       7.000000       7.000000   \n",
       "max       ...            590.000000     587.000000     577.000000   \n",
       "\n",
       "        w515_lacks  w516_combination  w517_sitting  w518_surface  \\\n",
       "count  2129.000000       2129.000000   2129.000000   2129.000000   \n",
       "mean      9.398309          9.175200      9.708783      8.807891   \n",
       "std      43.833064         42.403283     47.294120     39.341038   \n",
       "min       1.000000          1.000000      1.000000      1.000000   \n",
       "25%       3.000000          2.000000      3.000000      3.000000   \n",
       "50%       5.000000          5.000000      5.000000      5.000000   \n",
       "75%       7.000000          7.000000      7.000000      7.000000   \n",
       "max     598.000000        568.000000    599.000000    585.000000   \n",
       "\n",
       "       w519_fashion     w520_sit        class  \n",
       "count   2129.000000  2129.000000  2129.000000  \n",
       "mean       9.719587     9.307656     3.092532  \n",
       "std       46.185082    45.059367     1.395948  \n",
       "min        1.000000     1.000000     1.000000  \n",
       "25%        3.000000     2.000000     2.000000  \n",
       "50%        5.000000     4.000000     3.000000  \n",
       "75%        7.000000     6.000000     4.000000  \n",
       "max      600.000000   597.000000     5.000000  \n",
       "\n",
       "[8 rows x 521 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "nesw_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## Naive Bayes classification\n",
    "Now we want to fit a Gaussian Naive Bayes model to the cleaned dataset. You might want first to familiarise yourself with the [`GaussianNB`](http://scikit-learn.org/0.21/modules/generated/sklearn.naive_bayes.GaussianNB.html) class in `Sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 ==========\n",
    "\n",
    "Load the cleaned datasets `train_20news.csv` and `test_20news.csv` into pandas dataframes `news_train` and `news_test` respectively. Using pandas summary methods, confirm that the data is similar in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1_aaa</th>\n",
       "      <th>w2_pins</th>\n",
       "      <th>w3_kmr</th>\n",
       "      <th>w4_notion</th>\n",
       "      <th>w5_queens</th>\n",
       "      <th>w6_dwyer</th>\n",
       "      <th>w7_defenseman</th>\n",
       "      <th>w8_gld</th>\n",
       "      <th>w9_tocchet</th>\n",
       "      <th>w10_home</th>\n",
       "      <th>...</th>\n",
       "      <th>w512_constantly</th>\n",
       "      <th>w513_generate</th>\n",
       "      <th>w514_definite</th>\n",
       "      <th>w515_lacks</th>\n",
       "      <th>w516_combination</th>\n",
       "      <th>w517_sitting</th>\n",
       "      <th>w518_surface</th>\n",
       "      <th>w519_fashion</th>\n",
       "      <th>w520_sit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "      <td>2099.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.025727</td>\n",
       "      <td>1.017627</td>\n",
       "      <td>1.014769</td>\n",
       "      <td>1.008576</td>\n",
       "      <td>1.006193</td>\n",
       "      <td>1.010481</td>\n",
       "      <td>1.026203</td>\n",
       "      <td>1.024297</td>\n",
       "      <td>1.012387</td>\n",
       "      <td>1.084326</td>\n",
       "      <td>...</td>\n",
       "      <td>4.574083</td>\n",
       "      <td>4.464983</td>\n",
       "      <td>4.533111</td>\n",
       "      <td>4.557885</td>\n",
       "      <td>4.531682</td>\n",
       "      <td>4.514531</td>\n",
       "      <td>4.509290</td>\n",
       "      <td>4.521201</td>\n",
       "      <td>4.412577</td>\n",
       "      <td>3.091472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.280109</td>\n",
       "      <td>0.205208</td>\n",
       "      <td>0.172657</td>\n",
       "      <td>0.115206</td>\n",
       "      <td>0.078473</td>\n",
       "      <td>0.134174</td>\n",
       "      <td>0.618880</td>\n",
       "      <td>0.274219</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>0.491139</td>\n",
       "      <td>...</td>\n",
       "      <td>2.283028</td>\n",
       "      <td>2.273922</td>\n",
       "      <td>2.329654</td>\n",
       "      <td>2.292246</td>\n",
       "      <td>2.333558</td>\n",
       "      <td>2.259005</td>\n",
       "      <td>2.287548</td>\n",
       "      <td>2.295995</td>\n",
       "      <td>2.296504</td>\n",
       "      <td>1.395628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            w1_aaa      w2_pins       w3_kmr    w4_notion    w5_queens  \\\n",
       "count  2099.000000  2099.000000  2099.000000  2099.000000  2099.000000   \n",
       "mean      1.025727     1.017627     1.014769     1.008576     1.006193   \n",
       "std       0.280109     0.205208     0.172657     0.115206     0.078473   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max       8.000000     7.000000     5.000000     4.000000     2.000000   \n",
       "\n",
       "          w6_dwyer  w7_defenseman       w8_gld   w9_tocchet     w10_home  \\\n",
       "count  2099.000000    2099.000000  2099.000000  2099.000000  2099.000000   \n",
       "mean      1.010481       1.026203     1.024297     1.012387     1.084326   \n",
       "std       0.134174       0.618880     0.274219     0.153879     0.491139   \n",
       "min       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "25%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "50%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "75%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "max       4.000000      28.000000     5.000000     4.000000    16.000000   \n",
       "\n",
       "          ...       w512_constantly  w513_generate  w514_definite  \\\n",
       "count     ...           2099.000000    2099.000000    2099.000000   \n",
       "mean      ...              4.574083       4.464983       4.533111   \n",
       "std       ...              2.283028       2.273922       2.329654   \n",
       "min       ...              1.000000       1.000000       1.000000   \n",
       "25%       ...              3.000000       2.000000       3.000000   \n",
       "50%       ...              5.000000       4.000000       5.000000   \n",
       "75%       ...              7.000000       6.000000       7.000000   \n",
       "max       ...              8.000000       8.000000       8.000000   \n",
       "\n",
       "        w515_lacks  w516_combination  w517_sitting  w518_surface  \\\n",
       "count  2099.000000       2099.000000   2099.000000   2099.000000   \n",
       "mean      4.557885          4.531682      4.514531      4.509290   \n",
       "std       2.292246          2.333558      2.259005      2.287548   \n",
       "min       1.000000          1.000000      1.000000      1.000000   \n",
       "25%       3.000000          2.000000      3.000000      2.000000   \n",
       "50%       5.000000          5.000000      4.000000      4.000000   \n",
       "75%       7.000000          7.000000      6.000000      7.000000   \n",
       "max       8.000000          8.000000      8.000000      8.000000   \n",
       "\n",
       "       w519_fashion     w520_sit        class  \n",
       "count   2099.000000  2099.000000  2099.000000  \n",
       "mean       4.521201     4.412577     3.091472  \n",
       "std        2.295995     2.296504     1.395628  \n",
       "min        1.000000     1.000000     1.000000  \n",
       "25%        3.000000     2.000000     2.000000  \n",
       "50%        5.000000     4.000000     3.000000  \n",
       "75%        6.500000     6.000000     4.000000  \n",
       "max        8.000000     8.000000     5.000000  \n",
       "\n",
       "[8 rows x 521 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1_aaa</th>\n",
       "      <th>w2_pins</th>\n",
       "      <th>w3_kmr</th>\n",
       "      <th>w4_notion</th>\n",
       "      <th>w5_queens</th>\n",
       "      <th>w6_dwyer</th>\n",
       "      <th>w7_defenseman</th>\n",
       "      <th>w8_gld</th>\n",
       "      <th>w9_tocchet</th>\n",
       "      <th>w10_home</th>\n",
       "      <th>...</th>\n",
       "      <th>w512_constantly</th>\n",
       "      <th>w513_generate</th>\n",
       "      <th>w514_definite</th>\n",
       "      <th>w515_lacks</th>\n",
       "      <th>w516_combination</th>\n",
       "      <th>w517_sitting</th>\n",
       "      <th>w518_surface</th>\n",
       "      <th>w519_fashion</th>\n",
       "      <th>w520_sit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.00000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.015625</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.039062</td>\n",
       "      <td>1.015625</td>\n",
       "      <td>1.085938</td>\n",
       "      <td>...</td>\n",
       "      <td>4.757812</td>\n",
       "      <td>4.351562</td>\n",
       "      <td>4.593750</td>\n",
       "      <td>4.445312</td>\n",
       "      <td>4.53125</td>\n",
       "      <td>4.453125</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>4.421875</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>3.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088388</td>\n",
       "      <td>0.278847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>0.124507</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>0.291678</td>\n",
       "      <td>0.124507</td>\n",
       "      <td>0.281373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.201453</td>\n",
       "      <td>2.175706</td>\n",
       "      <td>2.438011</td>\n",
       "      <td>2.387001</td>\n",
       "      <td>2.31373</td>\n",
       "      <td>2.383868</td>\n",
       "      <td>2.390104</td>\n",
       "      <td>2.285212</td>\n",
       "      <td>2.370878</td>\n",
       "      <td>1.400840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.75000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.25000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           w1_aaa     w2_pins  w3_kmr  w4_notion   w5_queens    w6_dwyer  \\\n",
       "count  128.000000  128.000000   128.0      128.0  128.000000  128.000000   \n",
       "mean     1.007812    1.031250     1.0        1.0    1.007812    1.015625   \n",
       "std      0.088388    0.278847     0.0        0.0    0.088388    0.124507   \n",
       "min      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
       "25%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
       "50%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
       "75%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
       "max      2.000000    4.000000     1.0        1.0    2.000000    2.000000   \n",
       "\n",
       "       w7_defenseman      w8_gld  w9_tocchet    w10_home     ...      \\\n",
       "count     128.000000  128.000000  128.000000  128.000000     ...       \n",
       "mean        1.007812    1.039062    1.015625    1.085938     ...       \n",
       "std         0.088388    0.291678    0.124507    0.281373     ...       \n",
       "min         1.000000    1.000000    1.000000    1.000000     ...       \n",
       "25%         1.000000    1.000000    1.000000    1.000000     ...       \n",
       "50%         1.000000    1.000000    1.000000    1.000000     ...       \n",
       "75%         1.000000    1.000000    1.000000    1.000000     ...       \n",
       "max         2.000000    4.000000    2.000000    2.000000     ...       \n",
       "\n",
       "       w512_constantly  w513_generate  w514_definite  w515_lacks  \\\n",
       "count       128.000000     128.000000     128.000000  128.000000   \n",
       "mean          4.757812       4.351562       4.593750    4.445312   \n",
       "std           2.201453       2.175706       2.438011    2.387001   \n",
       "min           1.000000       1.000000       1.000000    1.000000   \n",
       "25%           3.000000       3.000000       2.000000    2.000000   \n",
       "50%           5.000000       4.000000       5.000000    4.000000   \n",
       "75%           7.000000       6.000000       7.000000    7.000000   \n",
       "max           8.000000       8.000000       8.000000    8.000000   \n",
       "\n",
       "       w516_combination  w517_sitting  w518_surface  w519_fashion    w520_sit  \\\n",
       "count         128.00000    128.000000    128.000000    128.000000  128.000000   \n",
       "mean            4.53125      4.453125      4.687500      4.421875    4.531250   \n",
       "std             2.31373      2.383868      2.390104      2.285212    2.370878   \n",
       "min             1.00000      1.000000      1.000000      1.000000    1.000000   \n",
       "25%             2.75000      2.000000      2.750000      2.000000    2.000000   \n",
       "50%             4.00000      5.000000      5.000000      4.000000    5.000000   \n",
       "75%             6.25000      7.000000      7.000000      6.000000    7.000000   \n",
       "max             8.00000      8.000000      8.000000      8.000000    8.000000   \n",
       "\n",
       "            class  \n",
       "count  128.000000  \n",
       "mean     3.078125  \n",
       "std      1.400840  \n",
       "min      1.000000  \n",
       "25%      2.000000  \n",
       "50%      3.000000  \n",
       "75%      4.000000  \n",
       "max      5.000000  \n",
       "\n",
       "[8 rows x 521 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your Code goes here:\n",
    "news_train = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'train_20news.csv'))\n",
    "news_test = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'test_20news.csv'))\n",
    "display(news_train.describe())\n",
    "display(news_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "[Text] Answer (in brief) the following two questions:\n",
    "1. What is the assumption behing the Naive Bayes Model?\n",
    "1. What would be the main issue we would have to face if we didn't make this assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1/2) ***Your answer goes here:***\n",
    "\n",
    "Naive Bayes assumes that each feature value in the same class are all independent, regardless of any correlation between the different features of that class. So the words are randomly distributed across the document.\n",
    "\n",
    "Without the assumption, we need to calculate a 520-dimensional Gaussian for each class, but with the assumption, we just need to calculate 520 1-dimensional Gaussians for each class, which requires less calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 ==========\n",
    "\n",
    "1. [Code] By using the `scatter_jitter` function, display a scatter plot of the features `w281_ico` and `w273_tek` for the **cleaned** dataset `news_train`. Set the jitter value to an appropriate value for visualisation. Label axes appropriately.\n",
    "1. [Text] What do you observe about these two features? Does this impact the validity of the Naive Bayes assumption? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "plt.xlabel('w281_ico')\n",
    "plt.ylabel('w273_tek')\n",
    "scatter_jitter(news_train['w281_ico'].values, news_train['w273_tek'].values, jitter=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 ==========\n",
    "1. [Text] What is a reasonable baseline against which to compare the classiffication performance? *Hint: What is the simplest classiffier you can think of?*. \n",
    "1. [Code] Estimate the baseline performance on the *training* data in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***\n",
    "\n",
    "A reasonable baseline to compare the classification performance is to predict all of the inputs as the class with the highest prior probability. For this dataset, points are split into five classes almost evenly, so the accuracy would be around 20% (there are 5 classes, 100/5=20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance when we guess all the data are class 1: 0.1729\n",
      "Performance when we guess all the data are class 2: 0.2006\n",
      "Performance when we guess all the data are class 3: 0.2010\n",
      "Performance when we guess all the data are class 4: 0.2130\n",
      "Performance when we guess all the data are class 5: 0.2125\n"
     ]
    }
   ],
   "source": [
    "# (2) # Your Code goes here:\n",
    "\n",
    "total = news_train.shape[0]\n",
    "print(\"Performance when we guess all the data are class 1: %.4f\" %(news_train[news_train['class']==1].shape[0]/total))\n",
    "print(\"Performance when we guess all the data are class 2: %.4f\" %(news_train[news_train['class']==2].shape[0]/total))\n",
    "print(\"Performance when we guess all the data are class 3: %.4f\" %(news_train[news_train['class']==3].shape[0]/total))\n",
    "print(\"Performance when we guess all the data are class 4: %.4f\" %(news_train[news_train['class']==4].shape[0]/total))\n",
    "print(\"Performance when we guess all the data are class 5: %.4f\" %(news_train[news_train['class']==5].shape[0]/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_2_5'></a>\n",
    "### ========== Question 2.5 ==========\n",
    "\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the cleaned dataset. \n",
    "\n",
    "1. [Code] Report the classification accuracy on the **training** dataset and plot a Confusion Matrix for the result (labelling the axes appropriately).\n",
    "\n",
    "1. [Text] Comment on the performance of the model. Is the accuracy a reasonable metric to use for this dataset?\n",
    "\n",
    "*Hint: You may make use of utility functions we provided, as well as an sklearn method for computing confusion matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "X = news_train.drop('class',axis=1)\n",
    "Y = news_train['class']\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy is: 0.8780\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'attributes_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-b7edbf9ea271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The classification accuracy is: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#attributes_names = ['alt.atheism','ibm.pc.hardware','mac.hardware','baseball','hockey']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattributes_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Normalised confusion matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'attributes_names' is not defined"
     ]
    }
   ],
   "source": [
    "# (2) # Your Code goes here:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The classification accuracy is: %.4f\" %(accuracy_score(Y, gnb.predict(X))))\n",
    "#attributes_names = ['alt.atheism','ibm.pc.hardware','mac.hardware','baseball','hockey']\n",
    "plot_confusion_matrix(confusion_matrix(Y, gnb.predict(X)),classes=attributes_names, norm=True,title='Normalised confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***\n",
    "\n",
    "We can find out the accuracy on the main diagonal is much high than other blocks, which is expected. This model performs reasonable for the dataset and reachs around 87.8% accuracy. Although accuracy on ibm.pc.hardware (0.646) is not as good as others but still the overall accuracy is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "[Text] Comment on the confusion matrix from the previous question. Does it look like what you would have expected? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***\n",
    "\n",
    "The confusion matrix gives a good represenatation of the performance of the model and is better than the baseline. This model performs reasonable for the dataset and reaches around 87.8% accuracy and the main diagonal is close to 1 as expected, so accuracy is a reasonable metric to use as it is a multi-class classifier and the correct classification for each class matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 ==========\n",
    "\n",
    "Now we want to evaluate the generalisation of the classifier on new (i.e. unseen data). \n",
    "\n",
    "1. [Code] Use the classifier you trained in Question [2.5](#question_2_5) (i.e. on the cleaned dataset) and test its performance on the test dataset. Display classification accuracy and plot a confusion matrix of the performance on the test data. \n",
    "\n",
    "1. [Code] Also, reevaluate the performance of the baseline on the test data.\n",
    "\n",
    "1. [Text] In a short paragraph (3-4 sentences) compare and comment on the results with (a) the training data and (b) the baseline (on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 ==========\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the original raw dataset (including the outliers) and test its performance on the **test** set. \n",
    "\n",
    "1. [Text] Comment on the output and explain why or why not cleaning affects the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.9 ==========\n",
    "\n",
    "In this exercise we have fitted a Gaussian Naive Bayes classifier to the data (i.e. the class conditional densities are Gaussians). However, this is not ideally suited to our dataset. Can you explain why this is so? what kind of Naive Bayes model would you employ to this kind of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_3'></a>\n",
    "# Question 3\n",
    "## Automobile Pricing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset\n",
    "This assignment is based on the automobile pricing dataset. Our goal will be to predict the price of automobiles based on various attributes. This data set consists of three types of entities: \n",
    "\n",
    "1. The specification of an automobile in terms of various characteristics \n",
    "\n",
    "1. Assigned insurance risk rating \n",
    "   * this rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuaries call this process ”symboling”. A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. \n",
    "\n",
    "1. Normalized losses in use as compared to other cars\n",
    "  * the third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year (avg_loss/car/year). \n",
    "\n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We removed any instances that had one or more missing values and randomized the data set. The resulting representation is much more compact and can be used directly to perform our experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into our problem, it is beneficial to get a feel for the data we are dealing with in the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_3_1'></a>\n",
    "### ========== Question 3.1 ==========\n",
    "\n",
    "Load the dataset `train_auto_numeric.csv` into a pandas DataFrame called `auto_numeric`. Using any suitable pandas functionality, \n",
    "1. [Code] summarise *and*\n",
    "1. [Text] comment upon\n",
    "\n",
    "the key features of the data. Show all your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>engine-power</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>mean-effective-pressure</th>\n",
       "      <th>torque</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164.0</td>\n",
       "      <td>99.8</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.2</td>\n",
       "      <td>54.3</td>\n",
       "      <td>8.85</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.00</td>\n",
       "      <td>102000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.52</td>\n",
       "      <td>57.68</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.0</td>\n",
       "      <td>99.4</td>\n",
       "      <td>162.4</td>\n",
       "      <td>66.4</td>\n",
       "      <td>54.3</td>\n",
       "      <td>15.18</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.00</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>47.39</td>\n",
       "      <td>59.59</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158.0</td>\n",
       "      <td>105.8</td>\n",
       "      <td>192.7</td>\n",
       "      <td>71.4</td>\n",
       "      <td>51.6</td>\n",
       "      <td>15.18</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.80</td>\n",
       "      <td>8.50</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3344.79</td>\n",
       "      <td>17710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>158.7</td>\n",
       "      <td>67.7</td>\n",
       "      <td>55.9</td>\n",
       "      <td>13.74</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.50</td>\n",
       "      <td>7.80</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44.74</td>\n",
       "      <td>68.97</td>\n",
       "      <td>23875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.0</td>\n",
       "      <td>101.2</td>\n",
       "      <td>176.8</td>\n",
       "      <td>64.8</td>\n",
       "      <td>54.3</td>\n",
       "      <td>8.67</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.80</td>\n",
       "      <td>8.80</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.78</td>\n",
       "      <td>53.48</td>\n",
       "      <td>16430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>194.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>190.9</td>\n",
       "      <td>71.4</td>\n",
       "      <td>58.7</td>\n",
       "      <td>8.67</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.90</td>\n",
       "      <td>22.50</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1330.28</td>\n",
       "      <td>16925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>188.0</td>\n",
       "      <td>101.2</td>\n",
       "      <td>176.8</td>\n",
       "      <td>64.8</td>\n",
       "      <td>54.3</td>\n",
       "      <td>26.58</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.19</td>\n",
       "      <td>9.00</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.19</td>\n",
       "      <td>377.06</td>\n",
       "      <td>20970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150.0</td>\n",
       "      <td>101.2</td>\n",
       "      <td>176.8</td>\n",
       "      <td>64.8</td>\n",
       "      <td>56.1</td>\n",
       "      <td>26.58</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.19</td>\n",
       "      <td>8.00</td>\n",
       "      <td>134000.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>57.37</td>\n",
       "      <td>48.20</td>\n",
       "      <td>21105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>121.0</td>\n",
       "      <td>88.4</td>\n",
       "      <td>141.1</td>\n",
       "      <td>60.3</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.03</td>\n",
       "      <td>9.50</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.33</td>\n",
       "      <td>25.08</td>\n",
       "      <td>5151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>98.0</td>\n",
       "      <td>104.3</td>\n",
       "      <td>155.9</td>\n",
       "      <td>68.3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.47</td>\n",
       "      <td>7.80</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>36.35</td>\n",
       "      <td>6295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>93.0</td>\n",
       "      <td>94.5</td>\n",
       "      <td>158.8</td>\n",
       "      <td>63.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.11</td>\n",
       "      <td>9.60</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>65.57</td>\n",
       "      <td>29.11</td>\n",
       "      <td>6575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>231.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>199.6</td>\n",
       "      <td>71.4</td>\n",
       "      <td>59.1</td>\n",
       "      <td>6.05</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.17</td>\n",
       "      <td>9.41</td>\n",
       "      <td>176000.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>20.15</td>\n",
       "      <td>5572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>174.6</td>\n",
       "      <td>63.8</td>\n",
       "      <td>49.4</td>\n",
       "      <td>6.05</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.23</td>\n",
       "      <td>21.50</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.28</td>\n",
       "      <td>152.80</td>\n",
       "      <td>6377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>118.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>157.3</td>\n",
       "      <td>63.8</td>\n",
       "      <td>50.8</td>\n",
       "      <td>7.10</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.39</td>\n",
       "      <td>7.60</td>\n",
       "      <td>102000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.80</td>\n",
       "      <td>73.88</td>\n",
       "      <td>7957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>197.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>192.7</td>\n",
       "      <td>71.4</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.05</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.90</td>\n",
       "      <td>22.50</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>47.53</td>\n",
       "      <td>41.38</td>\n",
       "      <td>6229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>148.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>157.3</td>\n",
       "      <td>63.8</td>\n",
       "      <td>50.6</td>\n",
       "      <td>6.05</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.23</td>\n",
       "      <td>9.40</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.59</td>\n",
       "      <td>136.33</td>\n",
       "      <td>6692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>148.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>157.3</td>\n",
       "      <td>63.8</td>\n",
       "      <td>50.6</td>\n",
       "      <td>6.05</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.23</td>\n",
       "      <td>9.40</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.07</td>\n",
       "      <td>488.77</td>\n",
       "      <td>7609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>110.0</td>\n",
       "      <td>103.3</td>\n",
       "      <td>174.6</td>\n",
       "      <td>64.6</td>\n",
       "      <td>59.8</td>\n",
       "      <td>11.47</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.46</td>\n",
       "      <td>8.50</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>76.69</td>\n",
       "      <td>33.05</td>\n",
       "      <td>8921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>118.0</td>\n",
       "      <td>95.9</td>\n",
       "      <td>173.2</td>\n",
       "      <td>66.3</td>\n",
       "      <td>50.2</td>\n",
       "      <td>22.65</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>7.00</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.17</td>\n",
       "      <td>116.32</td>\n",
       "      <td>12964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>137.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>144.6</td>\n",
       "      <td>63.9</td>\n",
       "      <td>50.8</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.60</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>51.27</td>\n",
       "      <td>33.41</td>\n",
       "      <td>6479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>103.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>202.6</td>\n",
       "      <td>63.9</td>\n",
       "      <td>50.8</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.20</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>78.82</td>\n",
       "      <td>23.08</td>\n",
       "      <td>6855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>137.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>170.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>4.85</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.07</td>\n",
       "      <td>10.10</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>93.73</td>\n",
       "      <td>19.60</td>\n",
       "      <td>5399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>101.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>150.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.20</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.20</td>\n",
       "      <td>64.24</td>\n",
       "      <td>6529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>101.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>150.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.41</td>\n",
       "      <td>9.20</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>81.94</td>\n",
       "      <td>23.87</td>\n",
       "      <td>7129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>87.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>163.4</td>\n",
       "      <td>68.8</td>\n",
       "      <td>55.9</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.07</td>\n",
       "      <td>9.50</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>4150.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.27</td>\n",
       "      <td>76.51</td>\n",
       "      <td>7295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>157.1</td>\n",
       "      <td>67.2</td>\n",
       "      <td>58.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.92</td>\n",
       "      <td>3.03</td>\n",
       "      <td>8.00</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.01</td>\n",
       "      <td>57.82</td>\n",
       "      <td>7295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>106.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>167.5</td>\n",
       "      <td>65.2</td>\n",
       "      <td>53.3</td>\n",
       "      <td>9.03</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.58</td>\n",
       "      <td>9.00</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>69.86</td>\n",
       "      <td>32.01</td>\n",
       "      <td>7895.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>106.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>175.4</td>\n",
       "      <td>65.2</td>\n",
       "      <td>53.3</td>\n",
       "      <td>9.03</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.58</td>\n",
       "      <td>9.00</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.27</td>\n",
       "      <td>434.03</td>\n",
       "      <td>9095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>118.0</td>\n",
       "      <td>102.4</td>\n",
       "      <td>175.4</td>\n",
       "      <td>68.4</td>\n",
       "      <td>56.2</td>\n",
       "      <td>9.03</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.00</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>68.07</td>\n",
       "      <td>33.85</td>\n",
       "      <td>8845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>85.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>175.4</td>\n",
       "      <td>62.5</td>\n",
       "      <td>54.1</td>\n",
       "      <td>9.03</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.58</td>\n",
       "      <td>9.00</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>231.93</td>\n",
       "      <td>10295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>134.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>176.2</td>\n",
       "      <td>65.6</td>\n",
       "      <td>56.3</td>\n",
       "      <td>18.54</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.50</td>\n",
       "      <td>10.10</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.92</td>\n",
       "      <td>158.36</td>\n",
       "      <td>11199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>134.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>176.2</td>\n",
       "      <td>65.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.54</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9.30</td>\n",
       "      <td>116000.0</td>\n",
       "      <td>4350.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.45</td>\n",
       "      <td>321.25</td>\n",
       "      <td>11549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>134.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>176.2</td>\n",
       "      <td>65.6</td>\n",
       "      <td>53.0</td>\n",
       "      <td>18.54</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9.30</td>\n",
       "      <td>116000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.32</td>\n",
       "      <td>122.30</td>\n",
       "      <td>17669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>161.0</td>\n",
       "      <td>99.1</td>\n",
       "      <td>168.9</td>\n",
       "      <td>66.2</td>\n",
       "      <td>49.4</td>\n",
       "      <td>11.47</td>\n",
       "      <td>2.99</td>\n",
       "      <td>3.27</td>\n",
       "      <td>9.60</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>50.81</td>\n",
       "      <td>45.78</td>\n",
       "      <td>8948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>65.0</td>\n",
       "      <td>102.4</td>\n",
       "      <td>175.6</td>\n",
       "      <td>66.5</td>\n",
       "      <td>54.9</td>\n",
       "      <td>9.03</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.35</td>\n",
       "      <td>22.50</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.75</td>\n",
       "      <td>73.48</td>\n",
       "      <td>10698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>74.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>163.4</td>\n",
       "      <td>66.5</td>\n",
       "      <td>53.7</td>\n",
       "      <td>11.47</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>8.70</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>67.79</td>\n",
       "      <td>35.61</td>\n",
       "      <td>9988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>65.0</td>\n",
       "      <td>102.4</td>\n",
       "      <td>175.6</td>\n",
       "      <td>66.5</td>\n",
       "      <td>54.9</td>\n",
       "      <td>11.47</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>8.70</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>551.75</td>\n",
       "      <td>10898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>65.0</td>\n",
       "      <td>102.4</td>\n",
       "      <td>175.6</td>\n",
       "      <td>66.5</td>\n",
       "      <td>53.9</td>\n",
       "      <td>11.47</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>8.70</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>97.39</td>\n",
       "      <td>25.24</td>\n",
       "      <td>11248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>110.0</td>\n",
       "      <td>88.4</td>\n",
       "      <td>173.2</td>\n",
       "      <td>66.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.57</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.64</td>\n",
       "      <td>9.30</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3132.33</td>\n",
       "      <td>16558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>197.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>183.5</td>\n",
       "      <td>67.7</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.57</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.35</td>\n",
       "      <td>9.30</td>\n",
       "      <td>161000.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>62.44</td>\n",
       "      <td>48.30</td>\n",
       "      <td>15998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>90.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>187.8</td>\n",
       "      <td>66.5</td>\n",
       "      <td>54.1</td>\n",
       "      <td>30.57</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.59</td>\n",
       "      <td>248.78</td>\n",
       "      <td>15690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>122.0</td>\n",
       "      <td>97.3</td>\n",
       "      <td>171.7</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>6.96</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.40</td>\n",
       "      <td>23.00</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>57.53</td>\n",
       "      <td>39.30</td>\n",
       "      <td>7775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>122.0</td>\n",
       "      <td>97.3</td>\n",
       "      <td>171.7</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>8.85</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>9.00</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>44.37</td>\n",
       "      <td>49.79</td>\n",
       "      <td>7975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>94.0</td>\n",
       "      <td>97.3</td>\n",
       "      <td>171.7</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>6.96</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.40</td>\n",
       "      <td>23.00</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>275.85</td>\n",
       "      <td>7995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>94.0</td>\n",
       "      <td>97.3</td>\n",
       "      <td>171.7</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>8.85</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>9.00</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.79</td>\n",
       "      <td>65.46</td>\n",
       "      <td>8195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>256.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>150.0</td>\n",
       "      <td>63.6</td>\n",
       "      <td>54.9</td>\n",
       "      <td>8.85</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>9.00</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>60.72</td>\n",
       "      <td>37.46</td>\n",
       "      <td>8495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>83.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>172.4</td>\n",
       "      <td>67.2</td>\n",
       "      <td>55.5</td>\n",
       "      <td>6.96</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.80</td>\n",
       "      <td>9.41</td>\n",
       "      <td>116000.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>63.23</td>\n",
       "      <td>36.68</td>\n",
       "      <td>9495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>94.0</td>\n",
       "      <td>97.3</td>\n",
       "      <td>171.7</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>8.85</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.60</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>54.41</td>\n",
       "      <td>42.27</td>\n",
       "      <td>9995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>256.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>165.7</td>\n",
       "      <td>63.6</td>\n",
       "      <td>51.4</td>\n",
       "      <td>8.85</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.64</td>\n",
       "      <td>9.10</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>960.49</td>\n",
       "      <td>9980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>90.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>173.5</td>\n",
       "      <td>66.2</td>\n",
       "      <td>56.2</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.24</td>\n",
       "      <td>4.17</td>\n",
       "      <td>8.80</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>75.27</td>\n",
       "      <td>38.69</td>\n",
       "      <td>12940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>74.0</td>\n",
       "      <td>104.3</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.50</td>\n",
       "      <td>114000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.16</td>\n",
       "      <td>104.04</td>\n",
       "      <td>13415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>125.0</td>\n",
       "      <td>94.5</td>\n",
       "      <td>171.7</td>\n",
       "      <td>66.5</td>\n",
       "      <td>54.5</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.07</td>\n",
       "      <td>9.20</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>9.58</td>\n",
       "      <td>306.48</td>\n",
       "      <td>15985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>74.0</td>\n",
       "      <td>104.3</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67.2</td>\n",
       "      <td>50.8</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.50</td>\n",
       "      <td>114000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47.20</td>\n",
       "      <td>64.45</td>\n",
       "      <td>16515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>103.0</td>\n",
       "      <td>104.3</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67.2</td>\n",
       "      <td>56.2</td>\n",
       "      <td>13.46</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.15</td>\n",
       "      <td>7.50</td>\n",
       "      <td>116000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1428.25</td>\n",
       "      <td>18420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>74.0</td>\n",
       "      <td>104.3</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>13.46</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.15</td>\n",
       "      <td>7.50</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.57</td>\n",
       "      <td>45.38</td>\n",
       "      <td>18950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>95.0</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.50</td>\n",
       "      <td>114000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>78.93</td>\n",
       "      <td>37.40</td>\n",
       "      <td>16845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>95.0</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.8</td>\n",
       "      <td>55.5</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>8.70</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.12</td>\n",
       "      <td>978.11</td>\n",
       "      <td>19045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>137.0</td>\n",
       "      <td>93.3</td>\n",
       "      <td>180.3</td>\n",
       "      <td>65.4</td>\n",
       "      <td>53.3</td>\n",
       "      <td>31.82</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.87</td>\n",
       "      <td>8.80</td>\n",
       "      <td>134000.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.54</td>\n",
       "      <td>92.57</td>\n",
       "      <td>21485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>95.0</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>18.17</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.40</td>\n",
       "      <td>23.00</td>\n",
       "      <td>106000.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>50.10</td>\n",
       "      <td>64.21</td>\n",
       "      <td>22470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>94.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>16.78</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.15</td>\n",
       "      <td>7.70</td>\n",
       "      <td>114000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>315.83</td>\n",
       "      <td>22625.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     normalized-losses  wheel-base  length  width  height  engine-size  bore  \\\n",
       "0                164.0        99.8   176.6   66.2    54.3         8.85  3.19   \n",
       "1                110.0        99.4   162.4   66.4    54.3        15.18  3.19   \n",
       "2                158.0       105.8   192.7   71.4    51.6        15.18  3.94   \n",
       "3                106.0        86.6   158.7   67.7    55.9        13.74  3.13   \n",
       "4                192.0       101.2   176.8   64.8    54.3         8.67  3.50   \n",
       "5                194.0       110.0   190.9   71.4    58.7         8.67  3.78   \n",
       "6                188.0       101.2   176.8   64.8    54.3        26.58  3.31   \n",
       "7                150.0       101.2   176.8   64.8    56.1        26.58  3.03   \n",
       "8                121.0        88.4   141.1   60.3    53.2         3.39  3.03   \n",
       "9                 98.0       104.3   155.9   68.3    52.0         6.05  3.31   \n",
       "10                93.0        94.5   158.8   63.6    52.0         6.05  3.03   \n",
       "11               231.0       113.0   199.6   71.4    59.1         6.05  3.78   \n",
       "12               118.0        93.7   174.6   63.8    49.4         6.05  2.97   \n",
       "13               118.0        93.7   157.3   63.8    50.8         7.10  3.03   \n",
       "14               197.0       110.0   192.7   71.4    58.7         6.05  3.78   \n",
       "15               148.0        93.7   157.3   63.8    50.6         6.05  2.97   \n",
       "16               148.0        93.7   157.3   63.8    50.6         6.05  2.97   \n",
       "17               110.0       103.3   174.6   64.6    59.8        11.47  3.34   \n",
       "18               118.0        95.9   173.2   66.3    50.2        22.65  3.60   \n",
       "19               137.0        86.6   144.6   63.9    50.8         6.30  2.91   \n",
       "20               103.0        98.4   202.6   63.9    50.8         6.30  2.91   \n",
       "21               137.0        93.7   170.2   64.0    52.6         4.85  2.91   \n",
       "22               101.0        93.7   150.0   64.0    52.6         6.30  2.91   \n",
       "23               101.0        93.7   150.0   64.0    52.6         6.30  2.91   \n",
       "24                87.0        96.5   163.4   68.8    55.9         6.30  2.54   \n",
       "25                78.0        96.5   157.1   67.2    58.3         6.30  2.92   \n",
       "26               106.0        96.5   167.5   65.2    53.3         9.03  3.15   \n",
       "27               106.0        96.5   175.4   65.2    53.3         9.03  3.15   \n",
       "28               118.0       102.4   175.4   68.4    56.2         9.03  3.46   \n",
       "29                85.0        96.5   175.4   62.5    54.1         9.03  3.15   \n",
       "..                 ...         ...     ...    ...     ...          ...   ...   \n",
       "129              134.0        98.4   176.2   65.6    56.3        18.54  3.34   \n",
       "130              134.0        98.4   176.2   65.6    52.0        18.54  3.01   \n",
       "131              134.0        98.4   176.2   65.6    53.0        18.54  3.58   \n",
       "132              161.0        99.1   168.9   66.2    49.4        11.47  2.99   \n",
       "133               65.0       102.4   175.6   66.5    54.9         9.03  3.27   \n",
       "134               74.0        93.1   163.4   66.5    53.7        11.47  3.31   \n",
       "135               65.0       102.4   175.6   66.5    54.9        11.47  3.31   \n",
       "136               65.0       102.4   175.6   66.5    53.9        11.47  3.31   \n",
       "137              110.0        88.4   173.2   66.4    52.0        30.57  3.94   \n",
       "138              197.0       102.9   183.5   67.7    52.0        30.57  3.27   \n",
       "139               90.0       104.5   187.8   66.5    54.1        30.57  3.27   \n",
       "140              122.0        97.3   171.7   65.5    55.7         6.96  3.01   \n",
       "141              122.0        97.3   171.7   65.5    55.7         8.85  3.19   \n",
       "142               94.0        97.3   171.7   65.5    55.7         6.96  3.01   \n",
       "143               94.0        97.3   171.7   65.5    55.7         8.85  3.19   \n",
       "144              256.0       115.6   150.0   63.6    54.9         8.85  3.19   \n",
       "145               83.0        93.7   172.4   67.2    55.5         6.96  3.19   \n",
       "146               94.0        97.3   171.7   65.5    55.7         8.85  3.19   \n",
       "147              256.0       104.5   165.7   63.6    51.4         8.85  2.97   \n",
       "148               90.0       113.0   173.5   66.2    56.2        16.78  3.24   \n",
       "149               74.0       104.3   188.8   67.2    57.5        16.78  3.78   \n",
       "150              125.0        94.5   171.7   66.5    54.5        16.78  3.34   \n",
       "151               74.0       104.3   188.8   67.2    50.8        16.78  3.78   \n",
       "152              103.0       104.3   188.8   67.2    56.2        13.46  3.62   \n",
       "153               74.0       104.3   188.8   67.2    57.5        13.46  3.62   \n",
       "154               95.0       109.1   188.8   68.9    55.5        16.78  3.78   \n",
       "155               95.0       109.1   188.8   68.8    55.5        16.78  3.78   \n",
       "156              137.0        93.3   180.3   65.4    53.3        31.82  3.50   \n",
       "157               95.0       109.1   188.8   68.9    55.5        18.17  3.01   \n",
       "158               94.0       108.0   188.8   68.9    55.5        16.78  3.78   \n",
       "\n",
       "     stroke  compression-ratio  engine-power  peak-rpm  city-mpg  highway-mpg  \\\n",
       "0      3.40              10.00      102000.0    5500.0      24.0         30.0   \n",
       "1      3.40               8.00      115000.0    5500.0      18.0         22.0   \n",
       "2      2.80               8.50       70000.0    4400.0      28.0         30.0   \n",
       "3      3.50               7.80      140000.0    5600.0      32.0         20.0   \n",
       "4      2.80               8.80      101000.0    5800.0      23.0         29.0   \n",
       "5      3.90              22.50      101000.0    6000.0      47.0         53.0   \n",
       "6      3.19               9.00      121000.0    4250.0      21.0         28.0   \n",
       "7      3.19               8.00      134000.0    4400.0      28.0         37.0   \n",
       "8      3.03               9.50       48000.0    5300.0      47.0         53.0   \n",
       "9      3.47               7.80       70000.0    5000.0      34.0         34.0   \n",
       "10     3.11               9.60       70000.0    5400.0      38.0         43.0   \n",
       "11     4.17               9.41      176000.0    6600.0      49.0         54.0   \n",
       "12     3.23              21.50       68000.0    5500.0      31.0         38.0   \n",
       "13     3.39               7.60      102000.0    5500.0      24.0         30.0   \n",
       "14     3.90              22.50      162000.0    6000.0      47.0         53.0   \n",
       "15     3.23               9.40       68000.0    5500.0      31.0         38.0   \n",
       "16     3.23               9.40       68000.0    5500.0      31.0         38.0   \n",
       "17     3.46               8.50       88000.0    5000.0      24.0         30.0   \n",
       "18     3.90               7.00      145000.0    5000.0      19.0         24.0   \n",
       "19     3.41               9.60       58000.0    4800.0      49.0         54.0   \n",
       "20     3.41               9.20       76000.0    6000.0      31.0         38.0   \n",
       "21     3.07              10.10       60000.0    5500.0      38.0         42.0   \n",
       "22     3.41               9.20       76000.0    6000.0      30.0         34.0   \n",
       "23     3.41               9.20       76000.0    6000.0      30.0         34.0   \n",
       "24     2.07               9.50      121000.0    4150.0      37.0         39.0   \n",
       "25     3.03               8.00       90000.0    5000.0      30.0         34.0   \n",
       "26     3.58               9.00       86000.0    5800.0      27.0         33.0   \n",
       "27     3.58               9.00       86000.0    5800.0      27.0         33.0   \n",
       "28     3.40              10.00      140000.0    5800.0      17.0         42.0   \n",
       "29     3.58               9.00       86000.0    5800.0      27.0         33.0   \n",
       "..      ...                ...           ...       ...       ...          ...   \n",
       "129    3.50              10.10      143000.0    5100.0      15.0         43.0   \n",
       "130    3.50               9.30      116000.0    4350.0      24.0         30.0   \n",
       "131    3.50               9.30      116000.0    5400.0      24.0         18.0   \n",
       "132    3.27               9.60       48000.0    4250.0      28.0         32.0   \n",
       "133    3.35              22.50       73000.0    4500.0      30.0         33.0   \n",
       "134    3.54               8.70       92000.0    4200.0      31.0         32.0   \n",
       "135    3.54               8.70       92000.0    4200.0      27.0         32.0   \n",
       "136    3.54               8.70       92000.0    4200.0      27.0         32.0   \n",
       "137    2.64               9.30      156000.0    4250.0      31.0         24.0   \n",
       "138    3.35               9.30      161000.0    6600.0      19.0         24.0   \n",
       "139    3.35               9.20      156000.0    5200.0      20.0         24.0   \n",
       "140    3.40              23.00      200000.0    4800.0      37.0         29.0   \n",
       "141    3.40               9.00       85000.0    5250.0      27.0         34.0   \n",
       "142    3.40              23.00       52000.0    4800.0      37.0         46.0   \n",
       "143    3.40               9.00       85000.0    5250.0      27.0         34.0   \n",
       "144    3.40               9.00       60000.0    5250.0      27.0         34.0   \n",
       "145    2.80               9.41      116000.0    4500.0      35.0         38.0   \n",
       "146    3.40               8.60      100000.0    5500.0      26.0         32.0   \n",
       "147    2.64               9.10       90000.0    5300.0      27.0         33.0   \n",
       "148    4.17               8.80       68000.0    5400.0      20.0         19.0   \n",
       "149    3.15               9.50      114000.0    5400.0      23.0         28.0   \n",
       "150    2.07               9.20      110000.0    6600.0      24.0         54.0   \n",
       "151    3.15               9.50      114000.0    5400.0      24.0         28.0   \n",
       "152    3.15               7.50      116000.0    5100.0      49.0         24.0   \n",
       "153    3.15               7.50      162000.0    5100.0      17.0         22.0   \n",
       "154    3.15               9.50      114000.0    5400.0      23.0         28.0   \n",
       "155    3.15               8.70      160000.0    5300.0      19.0         25.0   \n",
       "156    2.87               8.80      134000.0    4250.0      24.0         29.0   \n",
       "157    3.40              23.00      106000.0    4800.0      26.0         27.0   \n",
       "158    3.15               7.70      114000.0    5400.0      19.0         25.0   \n",
       "\n",
       "     mean-effective-pressure   torque    price  \n",
       "0                      40.52    57.68  13950.0  \n",
       "1                      47.39    59.59  17450.0  \n",
       "2                       0.85  3344.79  17710.0  \n",
       "3                      44.74    68.97  23875.0  \n",
       "4                      44.78    53.48  16430.0  \n",
       "5                       1.80  1330.28  16925.0  \n",
       "6                       7.19   377.06  20970.0  \n",
       "7                      57.37    48.20  21105.0  \n",
       "8                      59.33    25.08   5151.0  \n",
       "9                      51.55    36.35   6295.0  \n",
       "10                     65.57    29.11   6575.0  \n",
       "11                     93.12    20.15   5572.0  \n",
       "12                     12.28   152.80   6377.0  \n",
       "13                     28.80    73.88   7957.0  \n",
       "14                     47.53    41.38   6229.0  \n",
       "15                     14.59   136.33   6692.0  \n",
       "16                      4.07   488.77   7609.0  \n",
       "17                     76.69    33.05   8921.0  \n",
       "18                     24.17   116.32  12964.0  \n",
       "19                     51.27    33.41   6479.0  \n",
       "20                     78.82    23.08   6855.0  \n",
       "21                     93.73    19.60   5399.0  \n",
       "22                     30.20    64.24   6529.0  \n",
       "23                     81.94    23.87   7129.0  \n",
       "24                     26.27    76.51   7295.0  \n",
       "25                     35.01    57.82   7295.0  \n",
       "26                     69.86    32.01   7895.0  \n",
       "27                      5.27   434.03   9095.0  \n",
       "28                     68.07    33.85   8845.0  \n",
       "29                     10.23   231.93  10295.0  \n",
       "..                       ...      ...      ...  \n",
       "129                    16.92   158.36  11199.0  \n",
       "130                     8.45   321.25  11549.0  \n",
       "131                    24.32   122.30  17669.0  \n",
       "132                    50.81    45.78   8948.0  \n",
       "133                    33.75    73.48  10698.0  \n",
       "134                    67.79    35.61   9988.0  \n",
       "135                     4.38   551.75  10898.0  \n",
       "136                    97.39    25.24  11248.0  \n",
       "137                     0.95  3132.33  16558.0  \n",
       "138                    62.44    48.30  15998.0  \n",
       "139                    12.59   248.78  15690.0  \n",
       "140                    57.53    39.30   7775.0  \n",
       "141                    44.37    49.79   7975.0  \n",
       "142                     8.21   275.85   7995.0  \n",
       "143                    33.79    65.46   8195.0  \n",
       "144                    60.72    37.46   8495.0  \n",
       "145                    63.23    36.68   9495.0  \n",
       "146                    54.41    42.27   9995.0  \n",
       "147                     2.31   960.49   9980.0  \n",
       "148                    75.27    38.69  12940.0  \n",
       "149                    29.16   104.04  13415.0  \n",
       "150                     9.58   306.48  15985.0  \n",
       "151                    47.20    64.45  16515.0  \n",
       "152                     2.13  1428.25  18420.0  \n",
       "153                    69.57    45.38  18950.0  \n",
       "154                    78.93    37.40  16845.0  \n",
       "155                     3.12   978.11  19045.0  \n",
       "156                    32.54    92.57  21485.0  \n",
       "157                    50.10    64.21  22470.0  \n",
       "158                     9.70   315.83  22625.0  \n",
       "\n",
       "[159 rows x 16 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "auto_numeric = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'train_auto_numeric.csv'))\n",
    "auto_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.2 ==========\n",
    "\n",
    "We will now examine the attributes in some detail. Familiarise yourself with the concept of Correlation Coefficients (start from the Lecture on Generalisation and Evaluation).\n",
    "\n",
    "1. [Code] Analyse first the relationship between each attribute and price:\n",
    "  1. Compute the correlation coefficient between each attribute and price, *and*\n",
    "  1. Visualise the (pairwise) distribution of each attribute with price\n",
    "1. [Text] Given the above, which attributes do you feel may be most useful in predicting the price? (mention at least 5). How did you reach this conclusion? *Hint: which is the more useful of the above tools?*\n",
    "1. [Code] Now we will analyse the relationship between the attributes themselves. Use an appropriate pairwise visualisation tool to display graphically the relationship between each pair of attributes you selected in (2).\n",
    "1. [Text] Do any attributes exhibit significant correlations between one-another? (restrict your analysis to useful attributes identified above)\n",
    "1. [Text] Which attributes (give examples) would you consider removing if we wish to reduce the dimensionality of the problem and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "## Multivariate Linear Regression\n",
    "In this Section we will fit a Multivariate Linear Regression model (still using [`LinearRegression`](http://scikit-learn.org/0.21/modules/generated/sklearn.linear_model.LinearRegression.html)) to the dataset: i.e. we will now train a model with **multiple** explanatory variables and ascertain how they affect our ability to predict the retail price of a car. \n",
    "\n",
    "**N.B. In this question we will perform K-fold cross-validation using scikit's *KFold* class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_4_1'></a>\n",
    "### ========== Question 4.1  ==========\n",
    "\n",
    "K-fold cross-validation.\n",
    "\n",
    "1. [Text] What other technique for validation could we use (rather than K-Fold cross-validation)?\n",
    "\n",
    "1. [Text] Given the analysis you did on the automobile dataset in [Question 3](#question_3), what problem are we trying to solve by using K-Fold cross-validation?\n",
    "\n",
    "1. [Code] To solve this problem, we will use k-fold cross-validation to evaluate the performance of the regression model. By using Scikit-learn's [`KFold`](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html) class construct a 5-fold cross-validation object. Set `shuffle=True` and `random_state=0`. ***[Optional]*** *You may wish to visualise the training/validation indices per fold. The `split` method comes in handy in this case.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) # Your Code goes here:\n",
    "\n",
    "#k-fold cross-validation to evaluate the performance of the regression model\n",
    "kf = KFold(n_splits = 5, shuffle=True, random_state=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.1  ==========\n",
    "\n",
    "1. [Code] Train a Multi-Variate `LinearRegression` model on the original `auto_numeric` dataframe you loaded in [Question 3.1](#question_3_1), and evaluate it using the *KFold* instance you created in [Question 4.1](#question_4_1) (report RMSE and $R^2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of Determination(𝑅^2) on linear regression: 0.4861\n",
      "Root Mean Squared Error(RMSE) on linear regression: 4819.8046\n"
     ]
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "X = auto_numeric.drop('price',axis = 1)\n",
    "y = auto_numeric['price']\n",
    "price = auto_numeric['price'].values\n",
    "lr_new = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=1)\n",
    "predict_cv2 = cross_val_predict(lr_new, X,y, cv=kf)\n",
    "cod = r2_score(price, predict_cv2)\n",
    "rmse = np.sqrt(mean_squared_error(price, predict_cv2))\n",
    "print('Coefficient of Determination(𝑅^2) on linear regression: {:.4f}'.format(cod))\n",
    "print('Root Mean Squared Error(RMSE) on linear regression: {:.4f}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.2 ==========\n",
    "\n",
    "1. [Code] Examine the scatter plot of `engine-size` vs `price` (plot below)\n",
    "1. [Text] Why might this cause a problem for linear regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "plt.scatter(auto_numeric['engine-power'].values, auto_numeric['price'].values)\n",
    "plt.xlabel('engine-power')\n",
    "plt.ylabel('price')\n",
    "plt.title(\"price vs. engine-power\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***\n",
    "\n",
    "There are many outliers that may effect prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_4_3'></a>\n",
    "### ========== Question 4.3 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "In class we discussed ways of preprocessing features to improve performance in such cases.\n",
    "1. [Code] Transform the `engine-size` attribute using an appropriate technique from the lectures (document it in your code) and show the transformed data (scatter plot).\n",
    "1. [Code] Then retrain a (Multi-variate) LinearRegression Model (on all the attributes including the transformed `engine-size`) and report $R^2$ and RMSE. \n",
    "1. [Text] How has the performance of the model changed when compared to the previous result? and why so significantly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "auto_numeric_copy = auto_numeric.copy()\n",
    "plt.scatter(x=np.log(auto_numeric_copy['engine-size']), y=auto_numeric_copy['price'])\n",
    "plt.title(\"price vs. engine-size\")\n",
    "plt.xlabel('engine-size')\n",
    "plt.ylabel('price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Coefficient of Determination (R^2) on linear regression: 0.6359842143332417\n",
      "The Root Mean Squared Error (RMSE) on linear regression: 4056.6382974899443\n"
     ]
    }
   ],
   "source": [
    "# (2) # Your Code goes here:\n",
    "auto_numeric_copy['engine-size'] = np.log2(auto_numeric_copy['engine-size'])\n",
    "price = auto_numeric['price'].values\n",
    "\n",
    "X = auto_numeric_copy.drop('price',axis = 1)\n",
    "y = auto_numeric_copy['price']\n",
    "\n",
    "lr = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=1)\n",
    "pred_cv = cross_val_predict(lr, X, y, cv=kf)\n",
    "\n",
    "r2 = r2_score(price, pred_cv)\n",
    "rmse = np.sqrt(mean_squared_error(price, pred_cv))\n",
    "print('The Coefficient of Determination (R^2) on linear regression: {}'.format(r2))\n",
    "print('The Root Mean Squared Error (RMSE) on linear regression: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***\n",
    "\n",
    "The performance is better when compared to the previous results. \n",
    "The Coefficient of Determination(R^2) on linear regression, before: 0.4861, after: 0.6896.\n",
    "The Root Mean Squared Error(RMSE) on linear regression, before: 4819.8046, and after: 3745.9196.\n",
    "This increase in performance is because linear regression is sensitive to outliers and outliers exist in engine-size. The outliers have a relatively large value, the logarithm minimises the value, and the influence of the outliers would minimise with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.4 ==========\n",
    "\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "The simplicity of Linear Regression allows us to interpret the importance of certain features in predicting target variables. However this is not as straightforward as just reading off the coefficients of each of the attributes and ranking them in order of magnitude.\n",
    "\n",
    "1. [Text] Why is this? How can we *linearly* preprocess the attributes to allow for a comparison? Justify your answer.\n",
    "1. [Code] Perform the preprocessing you just mentioned on the transformed data-set from [Question 4.3](#question_4_3), retrain the Linear-Regressor and report the coefficients in a readable manner. *Tip: To simplify matters, you may abuse standard practice and train the model once on the entire data-set with no validation/test set.*\n",
    "1. [Text] Which are the three (3) most important features for predicting price under this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***\n",
    "\n",
    "Linear regression is extremely sensitive to outliers. For a better performance, we can remove records with values more than threshold or we can transform the data, this will disregard useless features. The threshold can be adjusted for more accuracy. To preprocess the data wecan normalize the data. A preprocessing utility from the sklearn class that can be used is the StandardScaler which computes the mean and standard deviation on a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.8675145166474896\n",
      "RMSE: 2447.3175520750374\n"
     ]
    }
   ],
   "source": [
    "# (2) # Your Code goes here:\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "#remove useless features\n",
    "feature_names = auto_numeric.columns[1:16].values\n",
    "X_tr= auto_numeric[feature_names]\n",
    "# preprocessing using normalize data\n",
    "normalizer = Normalizer()\n",
    "X_tr  = normalizer.fit_transform(X_tr)\n",
    "# preprocessing using normalize data\n",
    "standardiser = StandardScaler()\n",
    "X_tr  = standardiser.fit_transform(X_tr)\n",
    "y_tr = auto_numeric1['price']\n",
    "lr = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=1)\n",
    "pred_cv = cross_val_predict(lr, X_tr, y_tr, cv=kf)\n",
    "r2 = r2_score(price, pred_cv)\n",
    "rmse = np.sqrt(mean_squared_error(price, pred_cv))\n",
    "print('R^2: {}'.format(r2))\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.5 ==========\n",
    "\n",
    "In the lectures we discussed another form of extension to the basic linear-regression model: the introduction of basis functions. This method attempts to capture non-linearities in the input-output mapping.\n",
    "\n",
    "1. [Text] How would you choose the features to test higher-orders on? And how would you choose the best polynomial order for these features?\n",
    "1. [Code] Load the csv file `train_auto_nonlinear.csv` into a new dataframe (this is a standard version of the transformed data-set from [Question 3.3](#question_3_3)). Add a second-order basis to the two attributes `length` and `engine-power` and train a new LinearRegression model. Report the $R^2$ and RMSE performance.\n",
    "1. [Text] Comment on the result in relation to those in [Question 4.3](#question_4_3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
